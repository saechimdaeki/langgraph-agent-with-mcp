{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4031217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ec5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    context: list\n",
    "    answer: str\n",
    "\n",
    "graph_builder = StateGraph(AgentState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9995ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7934b3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "generate_prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def web_generate(state: AgentState) -> AgentState:\n",
    "    context = state[\"context\"]\n",
    "    query = state[\"query\"]\n",
    "    rag_chain = generate_prompt | llm | StrOutputParser()\n",
    "    response = rag_chain.invoke({\"question\": query, \"context\": context})\n",
    "    return {\"answer\": response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff2cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import TavilySearchResults\n",
    "\n",
    "tavily_search_tool = TavilySearchResults(\n",
    "    max_results=3,\n",
    "    search_depth=\"advanced\",\n",
    "    include_answer=True,\n",
    "    include_raw_content=True,\n",
    "    include_images=True\n",
    ")\n",
    "\n",
    "def web_search(state:AgentState):\n",
    "    query = state[\"query\"]\n",
    "    results = tavily_search_tool.invoke(query)\n",
    "    return {\"context\": results}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d134f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "basic_llm = ChatOllama(\n",
    "    model=\"deepseek-r1:1.5b\",\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "def basic_generate(state: AgentState):\n",
    "    query = state[\"query\"]\n",
    "    basic_llm_chain = basic_llm | StrOutputParser()\n",
    "    llm_response = basic_llm_chain.invoke(query)\n",
    "    return {\"answer\": llm_response}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1b604",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "class Route(BaseModel):\n",
    "    target: Literal['vector_store','llm','web_search'] = Field(\n",
    "        description = \"The target for the query to answer\"\n",
    "    )\n",
    "\n",
    "router_system_prompt = \"\"\"\n",
    "You are an export at routing a user's question to 'vector_store', 'llm' or 'web_search'.\n",
    "'vector_store' contains information about income tax up to 2025.\n",
    "if you think the question is simple enought use 'llm'\n",
    "if you think you need to search the web to answer the question use 'web_search'\n",
    "\"\"\"\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_messages([\n",
    "    ('system', router_system_prompt),\n",
    "    ('user','{query}')\n",
    "])\n",
    "\n",
    "router_llm = ChatOllama(model=\"deepseek-r1:1.5b\")\n",
    "structured_router_llm= router_llm.with_structured_output(Route)\n",
    "\n",
    "\n",
    "\n",
    "def router(state: AgentState):\n",
    "    query = state[\"query\"]\n",
    "    router_chain = router_prompt | structured_router_llm\n",
    "    route = router_chain.invoke({\"query\": query})\n",
    "    return route\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919f1ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from income_tax_graph import graph as income_tax_graph\n",
    "\n",
    "graph_builder.add_node('income_tax_agent', income_tax_graph)\n",
    "graph_builder.add_node('web_generate', web_generate)\n",
    "graph_builder.add_node('web_search', web_search)\n",
    "graph_builder.add_node('basic_generate', basic_generate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abe1775",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END\n",
    "\n",
    "graph_builder.add_conditional_edges(\n",
    "    START,\n",
    "    router,\n",
    "    {\n",
    "        'vector_store': 'income_tax_agent',\n",
    "        'llm': 'basic_generate',\n",
    "        'web_search': 'web_search'\n",
    "    }\n",
    ")\n",
    "\n",
    "graph_builder.add_edge('web_search', 'web_generate')\n",
    "graph_builder.add_edge('web_generate', END)\n",
    "graph_builder.add_edge('basic_generate', END)\n",
    "graph_builder.add_edge('income_tax_agent', END)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b1d9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dde7de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
